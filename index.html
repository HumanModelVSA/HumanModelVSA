<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Exploring Human-Model Alignment in Visual Social Attention During Help-and-Hinder Social Interaction Classification">
  <meta name="keywords" content="Visual Attention, Gaze, Social Interactions">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Exploring Human-Model Alignment in Visual Social Attention During Help-and-Hinder Social Interaction Classification</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robot_arm.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Exploring Human-Model Alignment in Visual Social Attention During Help-and-Hinder Social Interaction Classification</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Lucia Schiatti</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Guido Vallarino</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="">Sabrina M. Lopez</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="">Yen-Ling Kuo</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="">Matteo Moro</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="">Mengmi Zhang</a><sup>4</sup>,</span>
            <span class="author-block">
              <a href="">Monica Gori</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Alessio Del Bue</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Boris Katz</a><sup>5</sup>,</span>
            <span class="author-block">
              <a href="">Andrei Barbu</a><sup>5</sup>,</span>
            </span>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Istituto Italiano di Tecnologia</span>,
            <span class="author-block"><sup>2</sup>University of Genoa</span>,
            <span class="author-block"><sup>3</sup>University of Virginia</span>,
            <span class="author-block"><sup>4</sup>Nanyang Technological University</span>,
            <span class="author-block"><sup>5</sup>Massachusetts Institute of Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.14868"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.14868"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/kDd4p2xxf6g"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Gaze Dataset Zenodo Link. -->
              <span class="link-block">
                <a href="https://youtu.be/kDd4p2xxf6g"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">  
                      <i class="ai ai-zenodo ai-2x"></i>
                  </span>
                  <span> Data </span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/guidovalla/human_model_attention"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Artificial Neural Networks (ANNs) are considered the best models of human vision in many visual tasks. However, there is still a significant gap between humans and ANNs in visual tasks involving social perception and cognition. In this paper, we investigate the alignment between humans and different ANNs architectures, i.e., convolutional neural networks (CNNs) and transformers, during the classification of social interactions from videos.
            Specifically, we provide a novel dataset of videos and human gaze data during the classification of help and hinder social interactions, and we evaluate the human-model alignment in terms of classification accuracy and visual attention.
            We show that our proposed dataset and experimental protocol can enable comparison between different models' alignment to humans in terms of both static and dynamic visual attention.
            Our results suggest that a higher classification accuracy may be correlated with a higher similarity of the model's attention to human gaze-based saliency maps. 
            However, a better alignment of human-model attention is not always predictive of a better classification performance.
          </p>

          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video width="640" height="360" controls>
      <source src="https://github.com/HumanModelVSA/HumanModelVSA/raw/main/static/videos/Trial87_1_bc_WS_Help__r3__100.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section> 


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Can ANNs predict human visual attention during perception of social interactions?</h2>
    <div class="columns is-centered">
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-4">Physical (no interaction)</h2>
          <p>
            <!-- Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall. -->
          </p>
           <!--<video id="stacking" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/Trial1_1_ab_WS_Physical__r1__100.mp4"
                    type="video/mp4">
          </video>-->
          <iframe width="640" height="360" 
          src="https://youtube.com/shorts/wv-BzNkh9Xc?feature=share" 
          frameborder="0" 
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
          allowfullscreen
          muted>
          </iframe>
        </div>
    <div class="column">
      <div class="content"></div>
        <h2 class="title is-4">Help</h2>
          <p>
          </p>
          <video id="stacking" autoplay muted loop playsinline height="60%">
            <source src="./static/videos/Trial87_1_bc_WS_Help__r3__100.mp4"
                    type="video/mp4">
          </video>
        </div>
    <div class="column">
      <div class="content"></div>
        <h2 class="title is-4">Hinder</h2>
        <video id="plugging" autoplay muted loop playsinline height="60%">
          <source src="./static/videos/Trial26_1_ab_SW_Hinder__r1__100.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Real World Demonstration</h2>
    <video id="stacking" autoplay muted controls loop playsinline style="width: auto; height: auto;">
      <source src="./static/videos/Trial26_1_ab_SW_Hinder__r1__100.mp4" type="video/mp4">
    </video>
    <div class="columns is-centered">
  </div>

    


  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{lee2024diff,
      title={Diff-DAgger: Uncertainty Estimation with Diffusion Policy for Robotic Manipulation}, 
      author={Lee, Sung-Wook and Kang, Xuhui and Kuo, Yen-Ling},
      booktitle={International Conference on Robotics and Automation (ICRA)},
      year={2025}, 
}</code></pre>
  </div>
</section>


<!-- <footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>
</html>
